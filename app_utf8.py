import streamlit as st
import speech_recognition as sr
import pandas as pd
import numpy as np
from pytrends.request import TrendReq
import datetime
import time
import random 
import joblib # ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œìš©
from sklearn.metrics.pairwise import euclidean_distances # 'ìœ ì‚¬ ë‹¨ì–´' ê³„ì‚°ìš©
import requests # API í˜¸ì¶œìš©
import xml.etree.ElementTree as ET 

# -----------------------------------------------------------
# [ê¸°ëŠ¥ 1] í‘œì¤€ì–´ íŒë³„ í•¨ìˆ˜ (êµ­ë¦½êµ­ì–´ì› API)
# -----------------------------------------------------------
def check_is_standard_word(word):
    """
    êµ­ë¦½êµ­ì–´ì› í‘œì¤€êµ­ì–´ëŒ€ì‚¬ì „ APIë¥¼ ì¡°íšŒí•˜ì—¬
    í•´ë‹¹ ë‹¨ì–´ê°€ 'í‘œì¤€ì–´'ì¸ì§€(ì‚¬ì „ì— ë“±ì¬ë˜ì–´ ìˆëŠ”ì§€) í™•ì¸í•©ë‹ˆë‹¤.
    """
    # ğŸ”‘ ë°œê¸‰ë°›ì€ API í‚¤
    API_KEY = "C39F8A5DC5EEAE06C1307EDF6450E52B" 
    
    url = "https://stdict.korean.go.kr/api/search.do"
    
    params = {
        "key": API_KEY,
        "q": word,
        "req_type": "json",
        "advanced": "y",    
        "method": "exact"   
    }

    try:
        response = requests.get(url, params=params, timeout=5)
        if response.status_code == 200:
            data = response.json()
            if data and 'channel' in data and 'total' in data['channel']:
                count = int(data['channel']['total'])
                if count > 0:
                    return True # í‘œì¤€ì–´ì„
        return False # ì‚¬ì „ì— ì—†ìŒ (ì‹ ì¡°ì–´)
    except Exception:
        return False

# -----------------------------------------------------------
# [ê¸°ëŠ¥ 2] 4-Feature ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
# -----------------------------------------------------------
@st.cache_resource
def load_assets():
    try:
        # 1. 4-featureë¡œ í›ˆë ¨ëœ íŒŒì¼ ë¡œë“œ (íŒŒì¼ëª… í™•ì¸ í•„ìˆ˜)
        knn_model = joblib.load('knn_model.pkl')
        scaler = joblib.load('scaler.pkl')
        
        # 2. 4-featureê°€ í¬í•¨ëœ CSV ë¡œë“œ
        df = pd.read_csv('final_training_dataset.csv')
        df_train = df[df['Lifetime (Months)'] != 'Ongoing'].copy()
        
        # 3. [í•µì‹¬] ëª¨ë¸ì´ í•™ìŠµí•œ í”¼ì²˜ 4ê°œ (ìˆœì„œ ì¼ì¹˜ í•„ìˆ˜)
        features = ['Word_Length', 'Max_Rising_Slope', 'Initial_Volatility', 'Initial_Decay_Rate']
        
        # NaN ê°’ ì²˜ë¦¬ (í›ˆë ¨ ì‹œì™€ ë™ì¼í•˜ê²Œ 0ìœ¼ë¡œ ëŒ€ì²´)
        for col in features:
             if col in df_train.columns:
                 df_train[col] = df_train[col].fillna(0)
        
        # 4. 'ìœ ì‚¬ ë‹¨ì–´' ë¹„êµë¥¼ ìœ„í•´ ìŠ¤ì¼€ì¼ë§ëœ í›ˆë ¨ ë°ì´í„° X ì¤€ë¹„
        X_train_scaled = scaler.transform(df_train[features])
        
        # 5. ìœ ì‚¬ ë‹¨ì–´ì˜ 'ì´ë¦„' ëª©ë¡
        Word_names = df_train['Word'].values
        
        return df_train, knn_model, scaler, X_train_scaled, Word_names, features
    
    except FileNotFoundError:
        st.error("âŒ ERROR: ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.info("lifetime_calculator.pyì™€ k_nn_model.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ 4-Feature ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
        return None, None, None, None, None, None
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None, None, None, None, None, None

# -----------------------------------------------------------
# [ê¸°ëŠ¥ 3] 4-Feature ì‹¤ì‹œê°„ ê³„ì‚° í•¨ìˆ˜ (Data Leakage ë°©ì§€)
# -----------------------------------------------------------
def get_realtime_features(word):
    """
    ì…ë ¥ëœ ë‹¨ì–´ì˜ 'ìµœê·¼ 1ë…„ì¹˜' ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ 
    'ì´ˆê¸° ë‹¨ì„œ' í”¼ì²˜ 4ê°œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    try:
        pytrends = TrendReq(hl='ko-KR', tz=540)
        time.sleep(1 + random.uniform(0, 2)) # 429 ë°©ì§€ìš© ë”œë ˆì´

        # [í•µì‹¬] 1ë…„ì¹˜ ë°ì´í„°ë§Œ ìš”ì²­ (ë¯¸ë˜ ì •ë³´ ë°°ì œ)
        today = datetime.date.today()
        one_year_ago = today - datetime.timedelta(days=365)
        timeframe = f'{one_year_ago.strftime("%Y-%m-%d")} {today.strftime("%Y-%m-%d")}'
        
        pytrends.build_payload([word], cat=0, timeframe=timeframe, geo='KR')
        interest_df = pytrends.interest_over_time()
        
        if interest_df.empty or word not in interest_df.columns:
            st.warning(f"'{word}'ì— ëŒ€í•œ (ìµœê·¼ 1ë…„) íŠ¸ë Œë“œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None, None
            
        series = interest_df[word]
        
    except Exception as e:
        if "429" in str(e):
            st.error("âŒ Google Trends ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤ (429 Error). 1ë¶„ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        else:
            st.error(f"íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return None, None

    # --- 4-Feature ê³„ì‚° ë¡œì§ ---
    # 1. Word_Length
    word_len = len(word.replace(" ", ""))

    # 2. Max_Rising_Slope
    slopes = series.diff().fillna(0)
    max_rising_slope = slopes[slopes > 0].max()
    max_rising_slope = 0 if pd.isna(max_rising_slope) else max_rising_slope
    
    # 3. Initial_Volatility (ì´ˆê¸° ë³€ë™ì„±)
    initial_volatility = series.std()
    initial_volatility = 0 if pd.isna(initial_volatility) else initial_volatility
    
    # 4. Initial_Decay_Rate (ì´ˆê¸° í•˜ë½ ì†ë„)
    peak_index = series.idxmax()
    after_peak_series = series.loc[peak_index:]
    initial_decay_rate = 0
    if len(after_peak_series) > 1:
        initial_decay_rate = after_peak_series.mean()
        initial_decay_rate = 0 if pd.isna(initial_decay_rate) else initial_decay_rate

    # 4ê°€ì§€ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    final_features = [word_len, max_rising_slope, initial_volatility, initial_decay_rate]
    
    return final_features, series

# --- STT ë° UI í•¨ìˆ˜ ---
def on_stt_button_click():
    r = sr.Recognizer()
    with sr.Microphone() as source:
        st.info("ğŸ¤ ë§ˆì´í¬ì— ëŒ€ê³  ë§ì”€í•˜ì„¸ìš”... (3ì´ˆê°„ ë…¹ìŒ)")
        try:
            r.adjust_for_ambient_noise(source, duration=0.5)
            audio = r.listen(source, timeout=5, phrase_time_limit=3)
            text = r.recognize_google(audio, language='ko-KR')
            st.session_state.text = text 
            st.success(f"âœ… \"{text}\" ìŒì„± ì¸ì‹ ì„±ê³µ!")
        except sr.WaitTimeoutError:
            st.warning("âš ï¸ ìŒì„± ì…ë ¥ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except sr.UnknownValueError:
            st.error("âŒ ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜: {e}")

def load_css(file_name):
    try:
        with open(file_name, "r", encoding="utf-8") as f:
            st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)
    except:
        pass

# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def main():
    load_css("style.css")
    
    try:
        st.video("img/smoke.mp4", start_time=0)
    except:
        pass # ì˜ìƒ ì—†ìœ¼ë©´ íŒ¨ìŠ¤

    st.markdown('<h1 class="title-text"><span>â˜¯ï¸ë‹¨ì–´ ë©¸ë§ ì‹œê³„â˜¯ï¸</span></h1>', unsafe_allow_html=True)
    st.markdown("<p>ìŒì„±ìœ¼ë¡œ ì‹ ì¡°ì–´ë¥¼ ì…ë ¥í•˜ë©´, 4ê°€ì§€ 'ì´ˆê¸° ìœ í–‰ íŒ¨í„´'ì„ ë¶„ì„í•˜ì—¬ ìˆ˜ëª…ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.</p>", unsafe_allow_html=True)

    # 1. ìì‚° ë¡œë“œ
    assets = load_assets()
    df_train, knn_model, scaler, X_train_scaled, Word_names, features_list = assets
    
    if knn_model is None:
        return 

    # 2. STT ë²„íŠ¼
    st.button("Click to Speak", on_click=on_stt_button_click, use_container_width=True)

    # 3. ì‹¤í–‰ ë¡œì§
    if "text" in st.session_state and st.session_state.text:
        text = st.session_state.text
        st.markdown(f"<p class='user-input'>ì…ë ¥ëœ ë‹¨ì–´: \"{text}\"</p>", unsafe_allow_html=True)

        # ---------------------------------------------------------
        # [ë‹¨ê³„ 1] í‘œì¤€ì–´(ì˜ìƒ) íŒë³„
        # ---------------------------------------------------------
        is_standard = check_is_standard_word(text)

        if is_standard:
            st.balloons()
            st.success(f"âœ¨ '{text}'ì€(ëŠ”) í‘œì¤€êµ­ì–´ëŒ€ì‚¬ì „ì— ë“±ì¬ëœ 'í‘œì¤€ì–´'ì…ë‹ˆë‹¤.")
            st.markdown(f"""
                <div class='result-text' style='color: #4CAF50; font-size: 40px;'>
                    â™¾ï¸ ì˜ìƒ (Immortal)
                </div>
                <p class='sub-text' style='margin-top: 10px;'>
                    ì´ ë‹¨ì–´ëŠ” ìœ í–‰ì„ íƒ€ì§€ ì•Šê³ , ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ë¡œì„œ<br>
                    <b>ì˜ì›íˆ ìƒëª…ë ¥ì„ ìœ ì§€í•  ê²ƒ</b>ì…ë‹ˆë‹¤.
                </p>
            """, unsafe_allow_html=True)
            del st.session_state.text
            st.stop() 

        # ---------------------------------------------------------
        # [ë‹¨ê³„ 2] ì‹ ì¡°ì–´ ìˆ˜ëª… ì˜ˆì¸¡ (í‘œì¤€ì–´ê°€ ì•„ë‹ ê²½ìš°)
        # ---------------------------------------------------------
        live_features = None 
        live_series = None   
        is_new_word = False
        
        try:
            # (A) í•˜ì´ë¸Œë¦¬ë“œ ë¡œì§: DB ìš°ì„  ê²€ìƒ‰
            word_data_from_db = df_train[df_train['Word'] == text]
            
            if not word_data_from_db.empty:
                st.info("ğŸ’¡ í•™ìŠµëœ ë‹¨ì–´ì…ë‹ˆë‹¤. ì €ì¥ëœ ë°ì´í„°ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
                # DBì—ì„œ 4ê°œ í”¼ì²˜ ë¡œë“œ
                live_features = word_data_from_db[features_list].values.tolist()[0]
                is_new_word = False
            else:
                # (B) ì‹ ì¡°ì–´: API í˜¸ì¶œ (4ê°œ í”¼ì²˜ ê³„ì‚°)
                with st.spinner(f"ì‹ ì¡°ì–´ '{text}'ì˜ 'ì´ˆê¸° 1ë…„ íŒ¨í„´'ì„ ì‹¤ì‹œê°„ ë¶„ì„ ì¤‘..."):
                    live_features, live_series = get_realtime_features(text) 
                is_new_word = True

            if live_features is None:
                st.error("ë°ì´í„°ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                del st.session_state.text 
                st.stop() 
            
            # (C) ì˜ˆì¸¡ ë° ìŠ¤ì¼€ì¼ë§
            word_len, max_slope, volatility, decay_rate = live_features # 4ê°œ ì–¸íŒ¨í‚¹
            
            X_live_scaled = scaler.transform(np.array([live_features]))
            predicted_lifetime = knn_model.predict(X_live_scaled)
            predicted_months = int(np.round(predicted_lifetime[0]))
            
            # (D) ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸° (4ì°¨ì› ê±°ë¦¬)
            K = 5
            distances = euclidean_distances(X_live_scaled, X_train_scaled).flatten()
            
            if is_new_word:
                nearest_indices = np.argsort(distances)[:K]
            else:
                nearest_indices = np.argsort(distances)[1:K+1]
                
            nearby_words_list = Word_names[nearest_indices]

            # (E) ê²°ê³¼ í‘œì‹œ
            st.success(f"âœ… '{text}'ì˜ 'ì´ˆê¸° íŒ¨í„´' ë¶„ì„ ì™„ë£Œ!")
            
            # [ë””íœìŠ¤] 4ê°€ì§€ í”¼ì²˜ ìˆ˜ì¹˜ í‘œì‹œ
            st.markdown(f"""
                > (ê¸¸ì´: **{word_len}**, ì´ˆê¸° ê¸°ìš¸ê¸°: **{max_slope:.2f}**, 
                ì´ˆê¸° ë³€ë™ì„±: **{volatility:.2f}**, ì´ˆê¸° í•˜ë½ ì†ë„: **{decay_rate:.2f}**)
            """)
            
            if is_new_word and live_series is not None:
                st.info(f"'{text}'ì˜ ìµœê·¼ 1ë…„ íŠ¸ë Œë“œ")
                st.line_chart(live_series)

            st.subheader('ğŸ•°ï¸ ì˜ˆì¸¡ëœ ë©¸ë§ê¹Œì§€ ë‚¨ì€ ì‹œê°„')
            
            result_message = f"{predicted_months} ê°œì›”"
            status_text = "..." 
            if predicted_months <= 6 and predicted_months > 0:
                 result_message = f'<span style="color: red;">{predicted_months} ê°œì›”</span>'
                 status_text = "ğŸš¨ ì†Œë©¸ ì„ë°•! ê¸‰ê²©í•œ í•˜ë½ ì¶”ì„¸ì…ë‹ˆë‹¤."
            elif predicted_months == 0:
                 result_message = '<span>ì†Œë©¸ ì™„ë£Œ</span>'
                 status_text = "ğŸ’€ ìœ í–‰ì´ ëë‚¬ìŠµë‹ˆë‹¤."
            else:
                 result_message = f'<span style="color: #007BFF;">{predicted_months} ê°œì›”</span>'
                 status_text = "ğŸ“ˆ ì•„ì§ ìƒëª…ë ¥ì„ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤."

            # ì¹´ìš´íŠ¸ë‹¤ìš´
            result_placeholder = st.empty()
            start_tick = 60 
            if predicted_months > start_tick:
                start_point = start_tick
            else:
                start_point = max(predicted_months + 10, predicted_months)
            
            for i in range(start_point, predicted_months - 1, -1):
                result_placeholder.markdown(f"<div class=\"result-text\">{i}</div>", unsafe_allow_html=True)
                time.sleep(0.05) 

            result_placeholder.markdown(f"<div class=\"result-text\">{result_message}</div>", unsafe_allow_html=True)
            
            # [ë””íœìŠ¤] 4-Feature ë…¼ë¦¬ ì„¤ëª…
            st.markdown(f'<p class="sub-text" style="color: #AAA;">{status_text}</p>', unsafe_allow_html=True)
            st.markdown(f"""
                <p style='font-size: 16px; color: #E0E0E0;'>
                (ì˜ˆì¸¡ ê·¼ê±°: <b>'{text}'</b>ì˜ 'ì´ˆê¸° 4ëŒ€ ìœ í–‰ íŒ¨í„´'ì„
                ê¸°ì¡´ ë‹¨ì–´ (<b>{', '.join(nearby_words_list)}</b> ë“±)ì˜ 
                íŒ¨í„´ê³¼ ë¹„êµí•˜ì—¬ ìˆ˜ëª…ì„ ì˜ˆì¸¡)
                </p>
            """, unsafe_allow_html=True)

        except Exception as e:
            if "429" in str(e):
                st.error("âŒ Google Trends ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            else:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        del st.session_state.text

if __name__ == "__main__":
    st.set_page_config(page_title="ë‹¨ì–´ ë©¸ë§ ì‹œê³„", layout="centered") 
    main()

# ì•ˆê°œ íš¨ê³¼ CSS
st.markdown("""
<div class="fog-container">
  <div class="fog-img fog-img-first"></div>
  <div class="fog-img fog-img-second"></div>
</div>
""", unsafe_allow_html=True)