import streamlit as st
import speech_recognition as sr
import pandas as pd
import numpy as np
from pytrends.request import TrendReq
import datetime
import time
import random  
import joblib  # ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œìš©
from sklearn.metrics.pairwise import euclidean_distances  # 'ìœ ì‚¬ ë‹¨ì–´' ê³„ì‚°ìš©

# --- 0. [ìˆ˜ì •] 3-Feature ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ ---
@st.cache_resource
def load_assets():
    """
    3ê°œ í”¼ì²˜ë¡œ ë¯¸ë¦¬ í›ˆë ¨ëœ ëª¨ë¸(.pkl), ìŠ¤ì¼€ì¼ëŸ¬(.pkl),
    ê·¸ë¦¬ê³  DB ë° ìœ ì‚¬ ë‹¨ì–´ ë¹„êµì— ì‚¬ìš©í•  CSV íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    try:
        # 1. 3-featureë¡œ í›ˆë ¨ëœ íŒŒì¼ ë¡œë“œ
        knn_model = joblib.load('knn_model.pkl')
        scaler = joblib.load('scaler.pkl')
        
        # 2. 3-featureê°€ í¬í•¨ëœ CSV ë¡œë“œ
        df = pd.read_csv('final_training_dataset.csv')
        df_train = df[df['Lifetime (Months)'] != 'Ongoing'].copy()
        
        # 3. [í•µì‹¬ ìˆ˜ì •] ëª¨ë¸ì´ í•™ìŠµí•œ í”¼ì²˜ 3ê°œ (ìˆœì„œ ì¼ì¹˜ í•„ìˆ˜)
        features = ['Word_Length', 'Max_Rising_Slope', 'Peak_Value']
        
        # [ì¤‘ìš”] NaN ê°’ ì²˜ë¦¬ (í›ˆë ¨ ì‹œì™€ ë™ì¼)
        df_train['Max_Rising_Slope'] = df_train['Max_Rising_Slope'].fillna(0)
        df_train['Peak_Value'] = df_train['Peak_Value'].fillna(0) # Peak_Value ì²˜ë¦¬
        
        # 4. 'ìœ ì‚¬ ë‹¨ì–´' ë¹„êµë¥¼ ìœ„í•´ ìŠ¤ì¼€ì¼ë§ëœ í›ˆë ¨ ë°ì´í„° Xë¥¼ ë¯¸ë¦¬ ì¤€ë¹„
        # (scalerëŠ” 3ê°œ í”¼ì²˜ë¡œ í›ˆë ¨ë˜ì–´ ìˆì–´ì•¼ í•¨)
        X_train_scaled = scaler.transform(df_train[features])
        
        # 5. ìœ ì‚¬ ë‹¨ì–´ì˜ 'ì´ë¦„' ëª©ë¡
        Word_names = df_train['Word'].values
        
        # 6. ëª¨ë“  ìì‚° ë°˜í™˜ (features ë¦¬ìŠ¤íŠ¸ë„ í•¨ê»˜ ë°˜í™˜)
        return df_train, knn_model, scaler, X_train_scaled, Word_names, features
    
    except FileNotFoundError:
        st.error("âŒ ERROR: 'knn_model.pkl', 'scaler.pkl' ë˜ëŠ” 'final_training_dataset.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.info("ì„ í–‰ ë‹¨ê³„: 3-featureë¡œ lifetime_calculator.pyì™€ k_nn_model.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return None, None, None, None, None, None
    except Exception as e:
        st.error(f"âŒ ERROR: ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("ğŸš¨ í›ˆë ¨(k_nn_model.py)ê³¼ ì•±(app.py)ì˜ í”¼ì²˜ ê°œìˆ˜(3ê°œ)ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None, None, None, None, None, None

# --- 1. 3-Feature ì‹¤ì‹œê°„ ê³„ì‚° í•¨ìˆ˜ ---
def get_realtime_features_harmonized(word):
    try:
        pytrends = TrendReq(hl='ko-KR', tz=540)
        time.sleep(1 + random.uniform(0, 2))
        pytrends.build_payload([word], cat=0, timeframe='all', geo='KR')
        interest_df = pytrends.interest_over_time()
        if interest_df.empty or word not in interest_df.columns:
            st.warning(f"'{word}'ì— ëŒ€í•œ (ì „ì²´ ê¸°ê°„) íŠ¸ë Œë“œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None, None
        series = interest_df[word]
        # --- í”¼ì²˜ 3: Peak_Value ---
        peak_value = series.max()
        
        if peak_value == 0:
            max_rising_slope = 0.0
        else:
            # --- í”¼ì²˜ 2: Max_Rising_Slope  ---
            peak_date_index = series.idxmax()
            start_threshold = peak_value * 0.1 
            start_index = series[series >= start_threshold].first_valid_index()
            
            max_rising_slope = 0.0
            if start_index is not None and start_index < peak_date_index:
                rising_period = series.loc[start_index:peak_date_index]
                if len(rising_period) > 1:
                    max_slope = rising_period.diff().max()
                    if not pd.isna(max_slope):
                        max_rising_slope = max_slope
            max_rising_slope = max(0.0, max_rising_slope)

        # --- í”¼ì²˜ 1: Word_Length ---
        word_len = len(word.replace(" ", ""))

        # í•µì‹¬ ìˆ˜ì •] 3ê°œ í”¼ì²˜ ë°˜í™˜ (í›ˆë ¨ ìˆœì„œì™€ ë™ì¼: [ê¸¸ì´, ê¸°ìš¸ê¸°, ìµœëŒ€ê°’])
        final_features = [word_len, max_rising_slope, peak_value]
        
        return final_features, series

    except Exception as e:
        if "429" in str(e):
            st.error("âŒ ì‹¤ì‹œê°„ ë¶„ì„ API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤ (429 Error). 1ë¶„ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        else:
            st.error(f"íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return None, None


# --- 2. [V1] ìŒì„± ì¸ì‹ ì½œë°± í•¨ìˆ˜ (STT) (ë³€ê²½ ì—†ìŒ) ---
def on_stt_button_click():
    r = sr.Recognizer()
    with sr.Microphone() as source:
        st.info("ğŸ¤ ë§ˆì´í¬ì— ëŒ€ê³  ë§ì”€í•˜ì„¸ìš”... (3ì´ˆê°„ ë…¹ìŒ)")
        try:
            r.adjust_for_ambient_noise(source, duration=0.5)
            audio = r.listen(source, timeout=5, phrase_time_limit=3)
            text = r.recognize_google(audio, language='ko-KR')
            st.session_state.text = text 
            st.success(f"âœ… \"{text}\" ìŒì„± ì¸ì‹ ì„±ê³µ!")
        except sr.WaitTimeoutError:
            st.warning("âš ï¸ ìŒì„± ì…ë ¥ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except sr.UnknownValueError:
            st.error("âŒ ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except sr.RequestError as e:
            st.error(f"âŒ Google STT ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
        except Exception as e:
            st.error(f"âŒ STT ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")

# --- 3. CSS ë¡œë“œ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---
def load_css(file_name):
    try:
        with open(file_name, "r", encoding="utf-8") as f:
            st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)
    except FileNotFoundError:
        st.error(f"âŒ ERROR: 'style.css' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# --- 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (3-Feature ë°˜ì˜) ---
def main():
    load_css("style.css")
    
    try:
        st.video("img/smoke.mp4", start_time=0)
    except Exception as e:
        st.warning(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

    
    st.markdown('<h1 class="title-text"><span>â˜¯ï¸ë‹¨ì–´ ë©¸ë§ ì‹œê³„â˜¯ï¸</span></h1>', unsafe_allow_html=True)
    st.markdown("<p>ìŒì„±ìœ¼ë¡œ ì‹ ì¡°ì–´ë¥¼ ì…ë ¥í•˜ë©´, 3ê°€ì§€ 'ìœ í–‰ íŒ¨í„´'ì„ ì‹¤ì‹œê°„ ë¶„ì„í•˜ì—¬ ìˆ˜ëª…ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.</p>", unsafe_allow_html=True)

    # 1. 3-Feature ìì‚° ë¡œë“œ
    assets = load_assets()
    # features_list ì¶”ê°€
    df_train, knn_model, scaler, X_train_scaled, Word_names, features_list = assets
    
    if knn_model is None:
        return 

    # 2. STT ë²„íŠ¼
    st.button("Click to Speak", on_click=on_stt_button_click, use_container_width=True)

    # 3.  3-Feature STT ì™„ë£Œ í›„ ë¡œì§
    if "text" in st.session_state and st.session_state.text:
        text = st.session_state.text
        st.markdown(f"<p class='user-input'>ì…ë ¥ëœ ë‹¨ì–´: \"{text}\"</p>", unsafe_allow_html=True)

        live_features = None 
        live_series = None   
        is_new_word = False
        
        try:
            # --- (A) í•˜ì´ë¸Œë¦¬ë“œ ë¡œì§: DB(CSV) ìš°ì„  ê²€ìƒ‰ ---
            word_data_from_db = df_train[df_train['Word'] == text]
            
            if not word_data_from_db.empty:
                st.info("ğŸ’¡ í•™ìŠµëœ ë‹¨ì–´ì…ë‹ˆë‹¤. ì €ì¥ëœ ë°ì´í„°ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
                #  3ê°œ í”¼ì²˜(features_list)ë¥¼ DBì—ì„œ ë¡œë“œ
                live_features = word_data_from_db[features_list].values.tolist()[0]
                is_new_word = False
                
            else:
                # [ê²½ë¡œ 2: DBì— ë‹¨ì–´ê°€ ì—†ìŒ (ì‹ ì¡°ì–´)]
                with st.spinner(f"ì‹ ì¡°ì–´ '{text}'ì˜ 'ìœ í–‰ íŒ¨í„´'ì„ ì‹¤ì‹œê°„ ë¶„ì„ ì¤‘..."):
                    #  3ê°œ í”¼ì²˜ë¥¼ ì‹¤ì‹œê°„ ê³„ì‚°
                    live_features, live_series = get_realtime_features_harmonized(text) 
                is_new_word = True

            if live_features is None:
                st.error("ë°ì´í„°ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                del st.session_state.text 
                st.stop() 
            
            # --- (B) 3-Feature ìŠ¤ì¼€ì¼ë§ ë° ì˜ˆì¸¡ ---
            # 3ê°œ í”¼ì²˜ ì–¸íŒ¨í‚¹
            word_len_feature, max_rising_slope, peak_value = live_features
            
            # 2. 3-Feature ìŠ¤ì¼€ì¼ë§ (í›ˆë ¨ëœ scaler ì‚¬ìš©)
            X_live_scaled = scaler.transform(np.array([live_features]))

            # 3. 3-Feature ëª¨ë¸ ì˜ˆì¸¡
            predicted_lifetime = knn_model.predict(X_live_scaled)
            predicted_months = int(np.round(predicted_lifetime[0]))
            
            # 4. 'ìœ ì‚¬ ë‹¨ì–´' ì°¾ê¸° (3ì°¨ì› ê³µê°„ì—ì„œ)
            K = 5
            distances = euclidean_distances(X_live_scaled, X_train_scaled).flatten()
            
            if is_new_word:
                nearest_indices = np.argsort(distances)[:K]
            else:
                nearest_indices = np.argsort(distances)[1:K+1]
                
            nearby_words_list = Word_names[nearest_indices]

            # --- (C) ê²°ê³¼ í‘œì‹œ ë° ì¹´ìš´íŠ¸ë‹¤ìš´ ---
            st.success(f"âœ… '{text}'ì˜ 'ìœ í–‰ íŒ¨í„´' ë¶„ì„ ì™„ë£Œ!")
            #  3ê°œ í”¼ì²˜ í‘œì‹œ
            st.markdown(f"> (ë‹¨ì–´ ê¸¸ì´: **{word_len_feature}**, ìµœëŒ€ ê¸°ìš¸ê¸°: **{max_rising_slope:.2f}**, ìµœëŒ€ ê´€ì‹¬ë„: **{peak_value:.2f}**)")
            
            # [V2 ê¸°ëŠ¥] ì‹¤ì‹œê°„ ì°¨íŠ¸ í‘œì‹œ
            if is_new_word and live_series is not None:
                st.info(f"'{text}'ì˜ ì „ì²´ ê¸°ê°„ íŠ¸ë Œë“œ (Google Trends, KR)")
                st.line_chart(live_series)

            st.subheader('ğŸ•°ï¸ ì˜ˆì¸¡ëœ ë©¸ë§ê¹Œì§€ ë‚¨ì€ ì‹œê°„')
            
            # ê²°ê³¼ ë©”ì‹œì§€ ì„¤ì •
            result_message = f"{predicted_months} ê°œì›”"
            status_text = "..." 
            if predicted_months <= 6 and predicted_months > 0:
                    result_message = f'<span style="color: red;">{predicted_months} ê°œì›”</span>'
                    status_text = "ğŸš¨ ì†Œë©¸ ì„ë°•! ê¸‰ê²©í•œ í•˜ë½ ì¶”ì„¸ì…ë‹ˆë‹¤."
            elif predicted_months == 0:
                    result_message = '<span>ì†Œë©¸ ì™„ë£Œ</span>'
                    status_text = "ğŸ’€ ìœ í–‰ì´ ëë‚¬ìŠµë‹ˆë‹¤."
            else:
                    result_message = f'<span style="color: #007BFF;">{predicted_months} ê°œì›”</span>'
                    status_text = "ğŸ“ˆ ì•„ì§ ìƒëª…ë ¥ì„ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤."

            # ì¹´ìš´íŠ¸ë‹¤ìš´ ë¡œì§
            result_placeholder = st.empty()
            start_tick = 60 
            if predicted_months > start_tick:
                start_point = start_tick
            else:
                start_point = max(predicted_months + 10, predicted_months)
            
            for i in range(start_point, predicted_months - 1, -1):
                result_placeholder.markdown(f"<div class=\"result-text\">{i}</div>", unsafe_allow_html=True)
                time.sleep(0.05) 

            # ìµœì¢… ê²°ê³¼ ê³ ì •
            result_placeholder.markdown(f"<div class=\"result-text\">{result_message}</div>", unsafe_allow_html=True)
            
            # 3-Feature ë””íœìŠ¤ ë…¼ë¦¬
            st.markdown(f'<p class="sub-text" style="color: #AAA;">{status_text}</p>', unsafe_allow_html=True)
            st.markdown(f"""
                <p style='font-size: 16px; color: #E0E0E0;'>
                (ì˜ˆì¸¡ ê·¼ê±°: <b>'{text}'</b>ì˜ 3ê°€ì§€ íŒ¨í„´
                (ê¸¸ì´ {word_len_feature}, ê¸°ìš¸ê¸° {max_rising_slope:.2f}, ìµœëŒ€ê´€ì‹¬ë„ {peak_value:.2f})ì„
                ê¸°ì¡´ ë‹¨ì–´ (<b>{', '.join(nearby_words_list)}</b> ë“±)ì˜ 
                ìœ ì‚¬ íŒ¨í„´ê³¼ ë¹„êµí•˜ì—¬ ìˆ˜ëª…ì„ ì˜ˆì¸¡)
                </p>
            """, unsafe_allow_html=True)


        except Exception as e:
            if "429" in str(e):
                st.error("âŒ Google Trends ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            else:
                st.error(f"âŒ ì‹¤ì‹œê°„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                st.error(traceback.format_exc()) 
        
        del st.session_state.text

if __name__ == "__main__":
    st.set_page_config(page_title="ë‹¨ì–´ ë©¸ë§ ì‹œê³„", layout="centered") 
    main()

#  ì•ˆê°œ íš¨ê³¼ CSS
st.markdown("""
<div class="fog-container">
  <div class="fog-img fog-img-first"></div>
  <div class="fog-img fog-img-second"></div>
</div>
""", unsafe_allow_html=True)
